{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon Electronics Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import gzip\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/06 18:29:43 WARN Utils: Your hostname, MBP.local resolves to a loopback address: 127.0.0.1; using 192.168.0.29 instead (on interface en0)\n",
      "23/06/06 18:29:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/06 18:29:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create Spark session \n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta electronics file for price data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View first few rows of JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"category\": [\"Electronics\", \"Camera &amp; Photo\", \"Video Surveillance\", \"Surveillance Systems\", \"Surveillance DVR Kits\"], \"tech1\": \"\", \"description\": [\"The following camera brands and models have been tested for compatibility with GV-Software.\\nGeoVision \\tACTi \\tArecont Vision \\tAXIS \\tBosch \\tCanon\\nCNB \\tD-Link \\tEtroVision \\tHikVision \\tHUNT \\tIQEye\\nJVC \\tLG \\tMOBOTIX \\tPanasonic \\tPelco \\tSamsung\\nSanyo \\tSony \\tUDP \\tVerint \\tVIVOTEK \\t \\n \\nCompatible Standard and Protocol\\nGV-System also allows for integration with all other IP video devices compatible with ONVIF(V2.0), PSIA (V1.1) standards, or RTSP protocol.\\nONVIF \\tPSIA \\tRTSP \\t  \\t  \\t \\nNote: Specifications are subject to change without notice. Every effort has been made to ensure that the information on this Web site is accurate. No liability is assumed for incidental or consequential damages arising from the use of the information or products contained herein.\"], \"fit\": \"\", \"title\": \"Genuine Geovision 1 Channel 3rd Party NVR IP Software with USB Dongle Onvif PSIA\", \"also_buy\": [], \"tech2\": \"\", \"brand\": \"GeoVision\", \"feature\": [\"Genuine Geovision 1 Channel NVR IP Software\", \"Support 3rd Party IP Camera\", \"USB Dongle\"], \"rank\": [\">#3,092 in Tools &amp; Home Improvement &gt; Safety &amp; Security &gt; Home Security &amp; Surveillance &gt; Complete Surveillance Systems &gt; Surveillance DVR Kits\", \">#5,010 in Tools &amp; Home Improvement &gt; Safety &amp; Security &gt; Home Security &amp; Surveillance &gt; Surveillance Video Equipment\"], \"also_view\": [], \"main_cat\": \"Camera &amp; Photo\", \"similar_item\": \"\", \"date\": \"January 28, 2014\", \"price\": \"$65.00\", \"asin\": \"0011300000\", \"imageURL\": [\"https://images-na.ssl-images-amazon.com/images/I/411uoWa89KL._SS40_.jpg\"], \"imageURLHighRes\": [\"https://images-na.ssl-images-amazon.com/images/I/411uoWa89KL.jpg\"]}\n",
      "{\"category\": [\"Electronics\", \"Camera &amp; Photo\"], \"tech1\": \"\", \"description\": [\"This second edition of the Handbook of Astronomical Image Processing (HAIP) and its integral AIP for Windows 2.0 image processing software (AIP4Win2.0) addresses many important changes that have taken place in astronomical imaging since the publication of the first edition.  Today's affordable astro-imaging capable digital single-lens-reflex cameras (DSLRs), the growing power of personal computers, and the proliferation of telescopes and imaging accessories has brought imaging capabilities within the reach of practically every amateur astronomer - and this second edition of the Handbook plus AIP4Win 2.0 is ready, willing, and able to assist every observer in making great astronomical images.  In the Handbook, we amplified the original chapters on astronomical equipment and imaging techniques, revised our discussions of astrometry and photometry to reflect the steady growth in these scientific fields, and expanded tutorials in the back of the book to help you get up to speed quickly.  On the accompanying CDROM (found on the inside back cover) you will find hundreds of megabytes of sample images you can use to learn techniques such as image registration and stacking that guarantee good results even from those living with suburban and urban skies.  Also new are comprehensive chapters on color imaging with astronomical CCD cameras and processing color images from digital cameras, and photon-counting fundamentals every serious astro-imager needs to\"], \"fit\": \"\", \"title\": \"Books \\\"Handbook of Astronomical Image Processing\\\" with CD ROM, 2nd Edition, Hardcover Book by Berry &amp; Burnell\", \"also_buy\": [\"0999470906\"], \"tech2\": \"\", \"brand\": \"33 Books Co.\", \"feature\": [\"Detailed chapters cover these fundamental topics:\", \"Basic imaging: How the light that falls on your CCD becomes an image. Covers image formation, cameras, telescopes, detectors, sensor geometry, image capture,field of view, and angluar coverage.\", \"Counting Photons: \\\"Astronomy is about counting photons....\\\" Covers signal, noise, the signal-to-noise ratio, the Poisson and Gaussian distributions and whythey matter, making better pictures by summing images, and how dark frames and flat frames effect the signal and noise in your images.\", \"Digital Image Formats: Covers the file formats that astronomers use, including FITS, TIFF, BMP, and JPEG. Learn file format basics, how your image data is arranged inside the file on your computer's hard disk.\", \"Imaging Tools: All about sensors, optics, cameras, and telescopes. Explains how to calculate the field of view and resolution of your system, telescope optics for imagers, auxiliary optics, mounts, drives, tracking, filters, and how to recognize and correct common equipment problems.\"], \"rank\": [\">#55,933 in Camera &amp; Photo (See Top 100 in Camera &amp; Photo)\", \">#90,232 in Electronics &gt; Camera &amp; Photo\"], \"also_view\": [\"0943396670\", \"1138055360\", \"0999470906\"], \"main_cat\": \"Camera &amp; Photo\", \"similar_item\": \"\", \"date\": \"June 17, 2003\", \"price\": \"\", \"asin\": \"0043396828\", \"imageURL\": [\"https://images-na.ssl-images-amazon.com/images/I/51BjDGZhNvL._SS40_.jpg\"], \"imageURLHighRes\": [\"https://images-na.ssl-images-amazon.com/images/I/51BjDGZhNvL.jpg\"]}\n",
      "{\"category\": [\"Electronics\", \"eBook Readers &amp; Accessories\", \"eBook Readers\"], \"tech1\": \"\", \"description\": [\"A zesty tale. (Publishers Weekly)<br /><br />Garcia Aguilera creates human characters we can empathize with. (Fort Lauderdale Sun-Sentinel)<br /><br />Tropical politics, culture and identity...are at the center of One Hot Summer. (Fort Lauderdale Sun-Sentinel)\", \"\", \"Carolina Garcia-Aguilera is the author of the much-lauded <em>One Hot Summer</em> as well as the Lupe Solano mystery series. She was a private investigator for more than fifteen years before turning to writing full-time in 1996, The recipient of the Flamingo Award in 1999 and the Shamus Award in 2000, Ms. Garcia-Aguilera lives in Miami Beach.\", \"\", \"\", \"\"], \"fit\": \"\", \"title\": \"One Hot Summer\", \"also_buy\": [\"0425167798\", \"039914157X\"], \"tech2\": \"\", \"brand\": \"Visit Amazon's Carolina Garcia Aguilera Page\", \"feature\": [], \"rank\": \"3,105,177 in Books (\", \"also_view\": [], \"main_cat\": \"Books\", \"similar_item\": \"\", \"date\": \"\", \"price\": \"$11.49\", \"asin\": \"0060009810\", \"imageURL\": [], \"imageURLHighRes\": []}\n"
     ]
    }
   ],
   "source": [
    "N = 3\n",
    "with open(\"meta_Electronics.json\") as f:\n",
    "    for i in range(0,N):\n",
    "        print(f.readline(), end = '')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Generate Schema](https://preetranjan.github.io/pyspark-schema-generator/) and Load into spark dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField('category',ArrayType(StringType()),True),  \n",
    "StructField('tech1',StringType(),True),  \n",
    "StructField('description',ArrayType(StringType()),True),  \n",
    "StructField('fit',StringType(),True),  \n",
    "StructField('title',StringType(),True),  \n",
    "StructField('also_buy',ArrayType(StringType()),True),  \n",
    "StructField('tech2',StringType(),True),  \n",
    "StructField('brand',StringType(),True),  \n",
    "StructField('feature',ArrayType(StringType()),True),  \n",
    "StructField('rank',ArrayType(StringType()),True),  \n",
    "StructField('also_view',ArrayType(StringType()),True),  \n",
    "StructField('main_cat',StringType(),True),  \n",
    "StructField('similar_item',StringType(),True),  \n",
    "StructField('date',StringType(),True),  \n",
    "StructField('price',StringType(),True),  \n",
    "StructField('asin',StringType(),True),  \n",
    "StructField('imageURL',ArrayType(StringType()),True),  \n",
    "StructField('imageURLHighRes',ArrayType(StringType()),True)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schema required some editing, added `StringType()` as parameter in some `ArrayType()` fields that had `null` as parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- category: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- tech1: string (nullable = true)\n",
      " |-- description: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- fit: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- also_buy: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- tech2: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- feature: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- rank: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- also_view: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- main_cat: string (nullable = true)\n",
      " |-- similar_item: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- asin: string (nullable = true)\n",
      " |-- imageURL: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- imageURLHighRes: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "meta_elect_df = spark \\\n",
    "    .read \\\n",
    "    .format(\"json\") \\\n",
    "    .load(\"meta_Electronics.json\", schema = schema)\n",
    "\n",
    "meta_elect_df.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_elect_df.rdd.getNumPartitions()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:=======================================================> (80 + 2) / 82]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(786445, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print((meta_elect_df.count(), len(meta_elect_df.columns)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like a few hundred rows are missing (minor %). Website lists 786,868 products"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+---+--------------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+----------+--------------------+--------------------+\n",
      "|            category|tech1|         description|fit|               title|            also_buy|tech2|               brand|             feature|                rank|           also_view|            main_cat|        similar_item|              date|               price|      asin|            imageURL|     imageURLHighRes|\n",
      "+--------------------+-----+--------------------+---+--------------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+----------+--------------------+--------------------+\n",
      "|[Electronics, Cam...|     |[The following ca...|   |Genuine Geovision...|                  []|     |           GeoVision|[Genuine Geovisio...|[>#3,092 in Tools...|                  []|  Camera &amp; Photo|                    |  January 28, 2014|              $65.00|0011300000|[https://images-n...|[https://images-n...|\n",
      "|[Electronics, Cam...|     |[This second edit...|   |Books \"Handbook o...|        [0999470906]|     |        33 Books Co.|[Detailed chapter...|[>#55,933 in Came...|[0943396670, 1138...|  Camera &amp; Photo|                    |     June 17, 2003|                    |0043396828|[https://images-n...|[https://images-n...|\n",
      "|[Electronics, eBo...|     |[A zesty tale. (P...|   |      One Hot Summer|[0425167798, 0399...|     |Visit Amazon's Ca...|                  []|                null|                  []|               Books|                    |                  |              $11.49|0060009810|                  []|                  []|\n",
      "|[Electronics, eBo...|     |                  []|   |Hurray for Hattie...|[0060219521, 0060...|     |Visit Amazon's Di...|                  []|                null|[0060219521, 0060...|               Books|                    |                  |.a-section.a-spac...|0060219602|                  []|                  []|\n",
      "|[Electronics, eBo...|     |[&#8220;sex.lies....|   |sex.lies.murder.f...|                  []|     |Visit Amazon's Lo...|                  []|                null|                  []|               Books|                    |                  |              $13.95|0060786817|                  []|                  []|\n",
      "|[Electronics, eBo...|     |                [, ]|   |     College Physics|[0073049557, 0134...|     |Visit Amazon's Al...|                  []|                null|[0073512141, 0077...|               Books|                    |                  |                    |0070524076|                  []|                  []|\n",
      "|[Electronics, eBo...|     |[GIRL WITH A ONE-...|   |Girl with a One-t...|        [0330509691]|     |            ABBY LEE|                  []|                null|        [B0719LDQR1]|               Books|                    |                  |               $4.76|0091912407|                  []|                  []|\n",
      "|[Electronics, Por...|     |[Support system: ...|   |abcGoodefg&reg; 4...|[B01NAJ3KQB, B00W...|     |          Crazy Cart|[Package Content:...|[>#177,454 in Ele...|[B01NAJ3KQB, B00O...|     All Electronics| class=\"a-bordere...| December 28, 2012|                    |0101635370|[https://images-n...|[https://images-n...|\n",
      "|[Electronics, Hea...|     |[, <b>True High D...|   |Wireless Bluetoot...|                  []|     |     Enter The Arena|[Superb Sound Qua...|[>#950 in Cell Ph...|                  []|Home Audio & Theater|                    |  October 23, 2017|               $7.99|0132492776|[https://images-n...|[https://images-n...|\n",
      "|[Electronics, Com...|     |                  []|   |Kelby Training DV...|                  []|     |      Kelby Training|                  []|[>#932,732 in Com...|                  []|           Computers|                    |  December 9, 2011|                    |0132793040|[https://images-n...|[https://images-n...|\n",
      "|[Electronics, eBo...|     |[Claire Messud's ...|   |The Last Life: A ...|[0307596907, 0307...|     |Visit Amazon's Cl...|                  []|                null|[0393356051, 0307...|               Books|                    |                  |              $13.81|0151004714|                  []|                  []|\n",
      "|[Electronics, eBo...|     |[In this gleeful,...|   |        Lady Lazarus|                  []|     |Visit Amazon's An...|                  []|                null|        [1582436010]|               Books|                    |                  |               $5.79|0151014841|                  []|                  []|\n",
      "|[Electronics, Acc...|     |[, This book is a...|   |Practical Modern ...|[0736083723, 0205...|     |      John R. Wooden|                  []|                null|[1933538511, 0805...|               Books|                    |                  |                    |0205291252|                  []|                  []|\n",
      "|[Electronics, Com...|     |[Brand SAMSUNG Sp...|   |SAMSUNG Evo Plus ...|                  []|     |             Samsung|[64 GB MicroSDHC ...|[>#677,490 in Cel...|                  []|           Computers|                    |     July 22, 2016|                    |0246003235|[https://images-n...|[https://images-n...|\n",
      "|[Electronics, eBo...|     |                  []|   |BOOX Max Carta Er...|                  []|     |                BOOX|[Excellent Pinch ...|[>#264,331 in Ele...|[B077GVLMJN, B07B...|     All Electronics| class=\"a-bordere...|     June 30, 2016|                    |0285175270|[https://images-n...|[https://images-n...|\n",
      "|[Electronics, Cam...|     |[TDK Hi8 MP120 Pr...|   |TDK Hi8 MP120 Pre...|                  []|     |TDK Electronics Corp|                  []|[>#79 in Electron...|[B00009VX8C, B000...|Home Audio &amp; ...| class=\"a-bordere...|   August 22, 2014|              $48.99|0303532572|[https://images-n...|[https://images-n...|\n",
      "|[Electronics, Acc...|     |[While Frodo &amp...|   |The Lord of the R...|[B00009TB5G, B000...|     |                  WB|[Actors: Orlando ...|[>#359,908 in Ele...|                  []|Home Audio &amp; ...|                    |September 16, 2016|                    |0302643370|[https://images-n...|[https://images-n...|\n",
      "|[Electronics, eBo...|     |[Featuring a line...|   |Linen-Look Patter...|        [B000OWOS1Q]|     |Visit Amazon's Zo...|                  []|                null|                  []|               Books|                    |                  |                    |0310810515|                  []|                  []|\n",
      "|[Electronics, eBo...|     |[YA?Life in Thoma...|   |The Angel Doll: A...|[1878086804, 0060...|     |Visit Amazon's Je...|                  []|                null|[1878086804, B000...|               Books|                    |                  |               $3.98|0312171048|                  []|                  []|\n",
      "|[Electronics, eBo...|     |[Edgar-finalist H...|   |        Scared Money|[0312534868, 0312...|     |          James Hime|                  []|                null|                  []|               Books|                    |                  |              $12.78|0312331363|                  []|                  []|\n",
      "+--------------------+-----+--------------------+---+--------------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "meta_elect_df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick columns we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "|      asin|               brand|            main_cat|               price|               title|\n",
      "+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "|0011300000|           GeoVision|  Camera &amp; Photo|              $65.00|Genuine Geovision...|\n",
      "|0043396828|        33 Books Co.|  Camera &amp; Photo|                    |Books \"Handbook o...|\n",
      "|0060009810|Visit Amazon's Ca...|               Books|              $11.49|      One Hot Summer|\n",
      "|0060219602|Visit Amazon's Di...|               Books|.a-section.a-spac...|Hurray for Hattie...|\n",
      "|0060786817|Visit Amazon's Lo...|               Books|              $13.95|sex.lies.murder.f...|\n",
      "|0070524076|Visit Amazon's Al...|               Books|                    |     College Physics|\n",
      "|0091912407|            ABBY LEE|               Books|               $4.76|Girl with a One-t...|\n",
      "|0101635370|          Crazy Cart|     All Electronics|                    |abcGoodefg&reg; 4...|\n",
      "|0132492776|     Enter The Arena|Home Audio & Theater|               $7.99|Wireless Bluetoot...|\n",
      "|0132793040|      Kelby Training|           Computers|                    |Kelby Training DV...|\n",
      "|0151004714|Visit Amazon's Cl...|               Books|              $13.81|The Last Life: A ...|\n",
      "|0151014841|Visit Amazon's An...|               Books|               $5.79|        Lady Lazarus|\n",
      "|0205291252|      John R. Wooden|               Books|                    |Practical Modern ...|\n",
      "|0246003235|             Samsung|           Computers|                    |SAMSUNG Evo Plus ...|\n",
      "|0285175270|                BOOX|     All Electronics|                    |BOOX Max Carta Er...|\n",
      "|0303532572|TDK Electronics Corp|Home Audio &amp; ...|              $48.99|TDK Hi8 MP120 Pre...|\n",
      "|0302643370|                  WB|Home Audio &amp; ...|                    |The Lord of the R...|\n",
      "|0310810515|Visit Amazon's Zo...|               Books|                    |Linen-Look Patter...|\n",
      "|0312171048|Visit Amazon's Je...|               Books|               $3.98|The Angel Doll: A...|\n",
      "|0312331363|          James Hime|               Books|              $12.78|        Scared Money|\n",
      "+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols_to_use = ['asin', 'brand', 'main_cat', 'price', 'title']\n",
    "meta_elect_df.select(cols_to_use).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only want rows with prices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+---------+--------------------+\n",
      "|      asin|               brand|            main_cat|    price|               title|\n",
      "+----------+--------------------+--------------------+---------+--------------------+\n",
      "|0011300000|           GeoVision|  Camera &amp; Photo|   $65.00|Genuine Geovision...|\n",
      "|0060009810|Visit Amazon's Ca...|               Books|   $11.49|      One Hot Summer|\n",
      "|0060786817|Visit Amazon's Lo...|               Books|   $13.95|sex.lies.murder.f...|\n",
      "|0091912407|            ABBY LEE|               Books|    $4.76|Girl with a One-t...|\n",
      "|0132492776|     Enter The Arena|Home Audio & Theater|    $7.99|Wireless Bluetoot...|\n",
      "|0151004714|Visit Amazon's Cl...|               Books|   $13.81|The Last Life: A ...|\n",
      "|0151014841|Visit Amazon's An...|               Books|    $5.79|        Lady Lazarus|\n",
      "|0303532572|TDK Electronics Corp|Home Audio &amp; ...|   $48.99|TDK Hi8 MP120 Pre...|\n",
      "|0312171048|Visit Amazon's Je...|               Books|    $3.98|The Angel Doll: A...|\n",
      "|0312331363|          James Hime|               Books|   $12.78|        Scared Money|\n",
      "|0312364415|Visit Amazon's St...|               Books|    $3.25|A Stranger Lies T...|\n",
      "|0314185550|             BAR/BRI|               Books|    $3.00|BARBRI First Year...|\n",
      "|0323230008|           GeoVision|  Camera &amp; Photo|$2,080.00|Genuine Geovision...|\n",
      "|0375503757|Visit Amazon's Lu...|               Books|    $9.90|What She Saw...: ...|\n",
      "|0375505458|Visit Amazon's Wh...|               Books|    $7.00|A Collection of B...|\n",
      "|0380812916|Visit Amazon's Is...|               Books|    $9.07|Tokyo Suckerpunch...|\n",
      "|0385671539|       Brian Francis|               Books|   $32.95|       Natural Order|\n",
      "|0385666535|Visit Amazon's Da...|               Books|    $4.98|Incidents in the ...|\n",
      "|0446576476|Visit Amazon's Ch...|               Books|   $19.99|    Plenty Good Room|\n",
      "|0446697192|Visit Amazon's Zo...|               Books|   $17.99|Hollywood Is like...|\n",
      "+----------+--------------------+--------------------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "expr = '\\$.*' #Regex for anything that starts with a dollar sign ($)\n",
    "\n",
    "#Filter with regex\n",
    "meta_elect_df = meta_elect_df.filter(meta_elect_df.price.rlike(expr)).select(cols_to_use)\n",
    "meta_elect_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:=======================================================> (80 + 2) / 82]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(304323, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Check dimensions\n",
    "print((meta_elect_df.count(), len(meta_elect_df.columns)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like about half of the meta datset has prices."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Electronics data subset (5-core)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read first few lines of JSON: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"overall\": 5.0, \"vote\": \"67\", \"verified\": true, \"reviewTime\": \"09 18, 1999\", \"reviewerID\": \"AAP7PPBU72QFM\", \"asin\": \"0151004714\", \"style\": {\"Format:\": \" Hardcover\"}, \"reviewerName\": \"D. C. Carrad\", \"reviewText\": \"This is the best novel I have read in 2 or 3 years.  It is everything that fiction should be -- beautifully written, engaging, well-plotted and structured.  It has several layers of meanings -- historical, family,  philosophical and more -- and blends them all skillfully and interestingly.  It makes the American grad student/writers' workshop \\\"my parents were  mean to me and then my professors were mean to me\\\" trivia look  childish and silly by comparison, as they are.\\nAnyone who says this is an  adolescent girl's coming of age story is trivializing it.  Ignore them.  Read this book if you love literature.\\nI was particularly impressed with  this young author's grasp of the meaning and texture of the lost world of  French Algeria in the 1950's and '60's...particularly poignant when read in  1999 from another ruined and abandoned French colony, amid the decaying  buildings of Phnom Penh...\\nI hope the author will write many more books  and that her publishers will bring her first novel back into print -- I  want to read it.  Thank you, Ms. Messud, for writing such a wonderful work.\", \"summary\": \"A star is born\", \"unixReviewTime\": 937612800}\n",
      "{\"overall\": 3.0, \"vote\": \"5\", \"verified\": true, \"reviewTime\": \"10 23, 2013\", \"reviewerID\": \"A2E168DTVGE6SV\", \"asin\": \"0151004714\", \"style\": {\"Format:\": \" Kindle Edition\"}, \"reviewerName\": \"Evy\", \"reviewText\": \"Pages and pages of introspection, in the style of writers like Henry James.  I like this kind of  novels and the writer occasionally delights me with her descriptions and observations.  But it's way too repetitious for me and, I think, some parts could have been cut out while still preserving, and probably more tightly crystallizing, the themes and \\\"truths\\\" within the story.\\n\\nIt's a story I could relate to but I wish it hadn't been too tedious to read.\", \"summary\": \"A stream of consciousness novel\", \"unixReviewTime\": 1382486400}\n",
      "{\"overall\": 5.0, \"vote\": \"4\", \"verified\": false, \"reviewTime\": \"09 2, 2008\", \"reviewerID\": \"A1ER5AYS3FQ9O3\", \"asin\": \"0151004714\", \"style\": {\"Format:\": \" Paperback\"}, \"reviewerName\": \"Kcorn\", \"reviewText\": \"This is the kind of novel to read when you have time to lose yourself in a book for days, possibly weeks. I had to go back and reread it as soon as I finished it because it is so rich in historical and psychological  details, particularly themes centering on painful family dynamics and history.\\n\\nPotential readers of this novel by Claire Messud should know it isn't exactly a sunny, bouncy work. If you are looking for that you'll want to look elsewhere. It also moves between countries, including sections based in France, the U.S. and Algeria.\\n\\nThe focus of the novel is the LaBasse family and the story is told from the viewpoint of Sagesse, a woman who conveys the weight of a turbulent family history. Starting with Sagesse's grandfather, a man who owns the Hotel Bellevue in France,the family dynamics begin to play themselves out, connecting to larger themes of France and Algeria's history as well, with stunning consequences.\\n\\nSagesse's parents, Alex and Carol, reveal plenty about the kind of problems that can arise when there are cross-cultural challenges to face in a marriage. Carol, coming from America, marries Alex, but is unable to see the potential sacrifices and problems she'll face as she moves overseas to live with her husband...or the impact the marriage will have on her daughter and the rest of the family, including a handicapped son.\\n\\n It is a bit hard to categorize this book because it covers so many areas. It is part mystery because Sagesse is trying to separate truth from myth when figuring out her family's history. It is part suspense because of Sagesse's short love affair and a moment of violence (and I don't want to give the details of that moment or I'll spoil it for you). It is also a historical novel, rich with information about events in Algeria and France and the United States.\\n\\nFor me, also, this one centers on themes of personal identity and displacement,primarily for Sagesse, a woman trying to figure out where she belongs in the world and how to make peace with her difficult family as well as coming to grips with history's impact on her family.\", \"summary\": \"I'm a huge fan of the author and this one did not disappoint\", \"unixReviewTime\": 1220313600}\n"
     ]
    }
   ],
   "source": [
    "# Loading entire 4 gig file into 8GB of available ram...not good.....major slow downs.\n",
    "\n",
    "N = 3\n",
    "with open(\"Electronics_5.json\") as f:\n",
    "    for i in range(0,N):\n",
    "        print(f.readline(), end = '')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy first row of JSON output above and use [Schema Generator](https://preetranjan.github.io/pyspark-schema-generator/) to create schema below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField('overall',FloatType(),True),  # Changed to FloatType from StringType\n",
    "    StructField('vote',StringType(),True),  \n",
    "    StructField('verified',BooleanType(),True),  \n",
    "    StructField('reviewTime',StringType(),True),  \n",
    "    StructField('reviewerID',StringType(),True),  \n",
    "    StructField('asin',StringType(),True),  \n",
    "    StructField('style',StructType([StructField('Format:',StringType(),True)]),True),  \n",
    "    StructField('reviewerName',StringType(),True),  \n",
    "    StructField('reviewText',StringType(),True),  \n",
    "    StructField('summary',StringType(),True),  \n",
    "    StructField('unixReviewTime',IntegerType(),True)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use schema so spark does not have to infer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- overall: float (nullable = true)\n",
      " |-- vote: string (nullable = true)\n",
      " |-- verified: boolean (nullable = true)\n",
      " |-- reviewTime: string (nullable = true)\n",
      " |-- reviewerID: string (nullable = true)\n",
      " |-- asin: string (nullable = true)\n",
      " |-- style: struct (nullable = true)\n",
      " |    |-- Format:: string (nullable = true)\n",
      " |-- reviewerName: string (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- unixReviewTime: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5 core electronics data\n",
    "e5_core_df = spark \\\n",
    "    .read \\\n",
    "    .json(\"Electronics_5.json\", schema = schema)\n",
    "\n",
    "e5_core_df.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+--------+-----------+--------------+----------+-----------------+--------------------+--------------------+--------------------+--------------+\n",
      "|overall|vote|verified| reviewTime|    reviewerID|      asin|            style|        reviewerName|          reviewText|             summary|unixReviewTime|\n",
      "+-------+----+--------+-----------+--------------+----------+-----------------+--------------------+--------------------+--------------------+--------------+\n",
      "|    5.0|  67|    true|09 18, 1999| AAP7PPBU72QFM|0151004714|     { Hardcover}|        D. C. Carrad|This is the best ...|      A star is born|     937612800|\n",
      "|    3.0|   5|    true|10 23, 2013|A2E168DTVGE6SV|0151004714|{ Kindle Edition}|                 Evy|Pages and pages o...|A stream of consc...|    1382486400|\n",
      "|    5.0|   4|   false| 09 2, 2008|A1ER5AYS3FQ9O3|0151004714|     { Paperback}|               Kcorn|This is the kind ...|I'm a huge fan of...|    1220313600|\n",
      "|    5.0|  13|   false| 09 4, 2000|A1T17LMQABMBN5|0151004714|     { Hardcover}|     Caf Girl Writes|What gorgeous lan...|The most beautifu...|     968025600|\n",
      "|    3.0|   8|    true| 02 4, 2000|A3QHJ0FXK33OBE|0151004714|     { Hardcover}|    W. Shane Schmidt|I was taken in by...|A dissenting view...|     949622400|\n",
      "|    4.0|null|    true| 06 5, 2013|A3IYSOTP3HA77N|0380709473|{ Kindle Edition}|            B. Marks|I read this proba...|Above average mys...|    1370390400|\n",
      "|    5.0|null|    true|06 27, 2016|A11SXV34PZUQ5E|0380709473|{ Kindle Edition}|              Tom C.|I read every Perr...|        Lam is cool!|    1466985600|\n",
      "|    5.0|null|    true|07 30, 2015|A2AUQM1HT2D5T8|0380709473|{ Kindle Edition}|                 ema|I love this serie...|          Five Stars|    1438214400|\n",
      "|    5.0|null|    true|02 16, 2015|A3UD8JRWLX6SRX|0380709473|     { Paperback}|          Michael O.|         Great read!|          Five Stars|    1424044800|\n",
      "|    4.0|null|   false|11 21, 2013|A3MV1KKHX51FYT|0380709473|     { Paperback}|      Acute Observer|Crows Can't Count...|A Fast and Far Mo...|    1384992000|\n",
      "|    5.0|null|   false|07 14, 2009|A3LXXYBYUHZWS5|0446697192|     { Paperback}|        Monie Garcia|Fresh from Connec...|    A quick fun read|    1247529600|\n",
      "|    5.0|null|   false|07 10, 2009|A1X4L7AO1BXMHK|0446697192|     { Paperback}|             Deborah|I don't know abou...|  A Great Beach Read|    1247184000|\n",
      "|    3.0|null|   false| 09 2, 2009|A1Y9RUTH5GG3MU|0446697192|     { Paperback}|            D. Quinn|Obviously the pre...|Quick & fun summe...|    1251849600|\n",
      "|    4.0|null|   false|08 30, 2009| AAR8E3JF9K93P|0446697192|     { Paperback}|                Tina|I am very happy t...|Perfect beach/end...|    1251590400|\n",
      "|    4.0|   2|   false|07 29, 2009|A277GP2U2TXH51|0446697192|     { Paperback}|           grumpydan|\"Hollywood is Lik...|    Funny and Quirky|    1248825600|\n",
      "|    4.0|null|   false|07 28, 2009|A19KLUZ1XD3SRN|0446697192|     { Paperback}|Gaby at Starting ...|Synopsis:\\n\\nTayl...|Fun, satisfying, ...|    1248739200|\n",
      "|    2.0|  12|   false|04 14, 2014|A3J3BRHTDRFJ2G|0511189877|             null|            EJ Honda|This remote, for ...| Ergonomic nightmare|    1397433600|\n",
      "|    5.0|null|    true|05 31, 2017| A7362LXMQEM6W|0511189877|             null|      Online shopper|Worked out of the...|     Paid full price|    1496188800|\n",
      "|    2.0|null|   false|11 11, 2016| A2OSUEZJIN7BI|0511189877|             null|               Chris|I have an older U...|        Cannot Learn|    1478822400|\n",
      "|    5.0|null|   false|10 29, 2016|A1917RO9OGLVT9|0511189877|             null|             Spicoli|        Works great!|          Five Stars|    1477699200|\n",
      "+-------+----+--------+-----------+--------------+----------+-----------------+--------------------+--------------------+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "e5_core_df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e5_core_df.rdd.getNumPartitions()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:===========================================>            (25 + 7) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6739590, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print((e5_core_df.count(), len(e5_core_df.columns)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same number of rows as described on dataset website. Now to pick the columns that we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----------+--------------+--------------------+--------------------+\n",
      "|      asin|overall| reviewTime|    reviewerID|          reviewText|             summary|\n",
      "+----------+-------+-----------+--------------+--------------------+--------------------+\n",
      "|0151004714|    5.0|09 18, 1999| AAP7PPBU72QFM|This is the best ...|      A star is born|\n",
      "|0151004714|    3.0|10 23, 2013|A2E168DTVGE6SV|Pages and pages o...|A stream of consc...|\n",
      "|0151004714|    5.0| 09 2, 2008|A1ER5AYS3FQ9O3|This is the kind ...|I'm a huge fan of...|\n",
      "|0151004714|    5.0| 09 4, 2000|A1T17LMQABMBN5|What gorgeous lan...|The most beautifu...|\n",
      "|0151004714|    3.0| 02 4, 2000|A3QHJ0FXK33OBE|I was taken in by...|A dissenting view...|\n",
      "|0380709473|    4.0| 06 5, 2013|A3IYSOTP3HA77N|I read this proba...|Above average mys...|\n",
      "|0380709473|    5.0|06 27, 2016|A11SXV34PZUQ5E|I read every Perr...|        Lam is cool!|\n",
      "|0380709473|    5.0|07 30, 2015|A2AUQM1HT2D5T8|I love this serie...|          Five Stars|\n",
      "|0380709473|    5.0|02 16, 2015|A3UD8JRWLX6SRX|         Great read!|          Five Stars|\n",
      "|0380709473|    4.0|11 21, 2013|A3MV1KKHX51FYT|Crows Can't Count...|A Fast and Far Mo...|\n",
      "|0446697192|    5.0|07 14, 2009|A3LXXYBYUHZWS5|Fresh from Connec...|    A quick fun read|\n",
      "|0446697192|    5.0|07 10, 2009|A1X4L7AO1BXMHK|I don't know abou...|  A Great Beach Read|\n",
      "|0446697192|    3.0| 09 2, 2009|A1Y9RUTH5GG3MU|Obviously the pre...|Quick & fun summe...|\n",
      "|0446697192|    4.0|08 30, 2009| AAR8E3JF9K93P|I am very happy t...|Perfect beach/end...|\n",
      "|0446697192|    4.0|07 29, 2009|A277GP2U2TXH51|\"Hollywood is Lik...|    Funny and Quirky|\n",
      "|0446697192|    4.0|07 28, 2009|A19KLUZ1XD3SRN|Synopsis:\\n\\nTayl...|Fun, satisfying, ...|\n",
      "|0511189877|    2.0|04 14, 2014|A3J3BRHTDRFJ2G|This remote, for ...| Ergonomic nightmare|\n",
      "|0511189877|    5.0|05 31, 2017| A7362LXMQEM6W|Worked out of the...|     Paid full price|\n",
      "|0511189877|    2.0|11 11, 2016| A2OSUEZJIN7BI|I have an older U...|        Cannot Learn|\n",
      "|0511189877|    5.0|10 29, 2016|A1917RO9OGLVT9|        Works great!|          Five Stars|\n",
      "+----------+-------+-----------+--------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols_to_use = ['asin', 'overall', 'reviewTime', 'reviewerID', 'reviewText', 'summary']\n",
    "e5_core_df = e5_core_df.select(cols_to_use)\n",
    "e5_core_df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next up is to merge price with the data subset on `asin`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge data (primary key = asin)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Left join to get meta data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:======================================================> (80 + 2) / 82]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----------+--------------+--------------------+--------------------+--------------------+------------------+-------+--------------------+\n",
      "|      asin|overall| reviewTime|    reviewerID|          reviewText|             summary|               brand|          main_cat|  price|               title|\n",
      "+----------+-------+-----------+--------------+--------------------+--------------------+--------------------+------------------+-------+--------------------+\n",
      "|0446697192|    5.0|07 14, 2009|A3LXXYBYUHZWS5|Fresh from Connec...|    A quick fun read|Visit Amazon's Zo...|             Books| $17.99|Hollywood Is like...|\n",
      "|0446697192|    5.0|07 10, 2009|A1X4L7AO1BXMHK|I don't know abou...|  A Great Beach Read|Visit Amazon's Zo...|             Books| $17.99|Hollywood Is like...|\n",
      "|0446697192|    3.0| 09 2, 2009|A1Y9RUTH5GG3MU|Obviously the pre...|Quick & fun summe...|Visit Amazon's Zo...|             Books| $17.99|Hollywood Is like...|\n",
      "|0446697192|    4.0|08 30, 2009| AAR8E3JF9K93P|I am very happy t...|Perfect beach/end...|Visit Amazon's Zo...|             Books| $17.99|Hollywood Is like...|\n",
      "|0446697192|    4.0|07 29, 2009|A277GP2U2TXH51|\"Hollywood is Lik...|    Funny and Quirky|Visit Amazon's Zo...|             Books| $17.99|Hollywood Is like...|\n",
      "|0446697192|    4.0|07 28, 2009|A19KLUZ1XD3SRN|Synopsis:\\n\\nTayl...|Fun, satisfying, ...|Visit Amazon's Zo...|             Books| $17.99|Hollywood Is like...|\n",
      "|B000NOEDGK|    5.0| 09 5, 2013|A195EZSQDW3E21|Few electronic pr...|Fantastic Camera ...|               Nikon|Camera &amp; Photo|$189.99|Nikon D40x 10.2MP...|\n",
      "|B000NOEDGK|    5.0|06 23, 2013|A31AX66K0TX9RE|I got this item f...|        AWESOME DEAL|               Nikon|Camera &amp; Photo|$189.99|Nikon D40x 10.2MP...|\n",
      "|B000NOEDGK|    5.0|05 30, 2013| A1SQ6W1WC94E4|After using my D4...|   After 4 years use|               Nikon|Camera &amp; Photo|$189.99|Nikon D40x 10.2MP...|\n",
      "|B000NOEDGK|    5.0|03 13, 2013|A2FLZMVFJS231L|I've had this cam...|  Trustly little gem|               Nikon|Camera &amp; Photo|$189.99|Nikon D40x 10.2MP...|\n",
      "|B000NOEDGK|    5.0|08 23, 2011| A4EOR1UV8KU93|This is my second...|This camera does ...|               Nikon|Camera &amp; Photo|$189.99|Nikon D40x 10.2MP...|\n",
      "|B000NOEDGK|    1.0|07 23, 2011|A24YECGN7T5C98|My camera never w...|Unreliable, takes...|               Nikon|Camera &amp; Photo|$189.99|Nikon D40x 10.2MP...|\n",
      "|B000NOEDGK|    4.0|10 28, 2010|A1KKKKEPUETSGS|Light weight easy...|           Nice DSLR|               Nikon|Camera &amp; Photo|$189.99|Nikon D40x 10.2MP...|\n",
      "|B000NOEDGK|    5.0|10 12, 2010| A27BPO6VUGDGR|                bien|          Five Stars|               Nikon|Camera &amp; Photo|$189.99|Nikon D40x 10.2MP...|\n",
      "|B000NOEDGK|    5.0| 11 3, 2009|A2NVQZ4Y7Y0J1R|The D40x has been...|Still a great cam...|               Nikon|Camera &amp; Photo|$189.99|Nikon D40x 10.2MP...|\n",
      "|B00361EZUQ|    3.0| 11 2, 2010| ASRMT0X0G4UU2|I ordered a i5 im...|mid 2010 yellow t...|               Apple|         Computers|$277.00|Apple iMac MC509L...|\n",
      "|B00361EZUQ|    4.0|10 24, 2010|A3LB9PHFGNDC4B|This iMac replace...|      Great purchase|               Apple|         Computers|$277.00|Apple iMac MC509L...|\n",
      "|B00361EZUQ|    5.0| 10 2, 2010| ANW52NKL43W1S|I do a lot of pho...|First Time iMac B...|               Apple|         Computers|$277.00|Apple iMac MC509L...|\n",
      "|B00361EZUQ|    5.0| 09 6, 2010| A36C22168X6UQ|After 25+ years o...|My first Mac, lov...|               Apple|         Computers|$277.00|Apple iMac MC509L...|\n",
      "|B00361EZUQ|    4.0| 09 6, 2010|A1JUBRIZMO0RJV|I anxiously await...|Solid, Quick Mac,...|               Apple|         Computers|$277.00|Apple iMac MC509L...|\n",
      "+----------+-------+-----------+--------------+--------------------+--------------------+--------------------+------------------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "elect_df = e5_core_df.join(meta_elect_df, on = 'asin', how = 'left')\n",
    "elect_df.show()\n",
    "# print('5 core dataset: ', (e5_core_df.count(), len(e5_core_df.columns)))\n",
    "# print('Joined dataset: ', (elect_df.count(), len(elect_df.columns)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean dates (reviewTime):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:======================================================> (80 + 2) / 82]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+----------+--------------+--------------------+--------------------+--------------------+------------------+-------+--------------------+\n",
      "|      asin|overall|reviewTime|    reviewerID|          reviewText|             summary|               brand|          main_cat|  price|               title|\n",
      "+----------+-------+----------+--------------+--------------------+--------------------+--------------------+------------------+-------+--------------------+\n",
      "|0446697192|    5.0|07-14-2009|A3LXXYBYUHZWS5|Fresh from Connec...|    A quick fun read|Visit Amazon's Zo...|             Books| $17.99|Hollywood Is like...|\n",
      "|0446697192|    5.0|07-10-2009|A1X4L7AO1BXMHK|I don't know abou...|  A Great Beach Read|Visit Amazon's Zo...|             Books| $17.99|Hollywood Is like...|\n",
      "|0446697192|    3.0| 09-2-2009|A1Y9RUTH5GG3MU|Obviously the pre...|Quick & fun summe...|Visit Amazon's Zo...|             Books| $17.99|Hollywood Is like...|\n",
      "|0446697192|    4.0|08-30-2009| AAR8E3JF9K93P|I am very happy t...|Perfect beach/end...|Visit Amazon's Zo...|             Books| $17.99|Hollywood Is like...|\n",
      "|0446697192|    4.0|07-29-2009|A277GP2U2TXH51|\"Hollywood is Lik...|    Funny and Quirky|Visit Amazon's Zo...|             Books| $17.99|Hollywood Is like...|\n",
      "|0446697192|    4.0|07-28-2009|A19KLUZ1XD3SRN|Synopsis:\\n\\nTayl...|Fun, satisfying, ...|Visit Amazon's Zo...|             Books| $17.99|Hollywood Is like...|\n",
      "|B000NOEDGK|    5.0| 09-5-2013|A195EZSQDW3E21|Few electronic pr...|Fantastic Camera ...|               Nikon|Camera &amp; Photo|$189.99|Nikon D40x 10.2MP...|\n",
      "|B000NOEDGK|    5.0|06-23-2013|A31AX66K0TX9RE|I got this item f...|        AWESOME DEAL|               Nikon|Camera &amp; Photo|$189.99|Nikon D40x 10.2MP...|\n",
      "|B000NOEDGK|    5.0|05-30-2013| A1SQ6W1WC94E4|After using my D4...|   After 4 years use|               Nikon|Camera &amp; Photo|$189.99|Nikon D40x 10.2MP...|\n",
      "|B000NOEDGK|    5.0|03-13-2013|A2FLZMVFJS231L|I've had this cam...|  Trustly little gem|               Nikon|Camera &amp; Photo|$189.99|Nikon D40x 10.2MP...|\n",
      "|B000NOEDGK|    5.0|08-23-2011| A4EOR1UV8KU93|This is my second...|This camera does ...|               Nikon|Camera &amp; Photo|$189.99|Nikon D40x 10.2MP...|\n",
      "|B000NOEDGK|    1.0|07-23-2011|A24YECGN7T5C98|My camera never w...|Unreliable, takes...|               Nikon|Camera &amp; Photo|$189.99|Nikon D40x 10.2MP...|\n",
      "|B000NOEDGK|    4.0|10-28-2010|A1KKKKEPUETSGS|Light weight easy...|           Nice DSLR|               Nikon|Camera &amp; Photo|$189.99|Nikon D40x 10.2MP...|\n",
      "|B000NOEDGK|    5.0|10-12-2010| A27BPO6VUGDGR|                bien|          Five Stars|               Nikon|Camera &amp; Photo|$189.99|Nikon D40x 10.2MP...|\n",
      "|B000NOEDGK|    5.0| 11-3-2009|A2NVQZ4Y7Y0J1R|The D40x has been...|Still a great cam...|               Nikon|Camera &amp; Photo|$189.99|Nikon D40x 10.2MP...|\n",
      "|B00361EZUQ|    3.0| 11-2-2010| ASRMT0X0G4UU2|I ordered a i5 im...|mid 2010 yellow t...|               Apple|         Computers|$277.00|Apple iMac MC509L...|\n",
      "|B00361EZUQ|    4.0|10-24-2010|A3LB9PHFGNDC4B|This iMac replace...|      Great purchase|               Apple|         Computers|$277.00|Apple iMac MC509L...|\n",
      "|B00361EZUQ|    5.0| 10-2-2010| ANW52NKL43W1S|I do a lot of pho...|First Time iMac B...|               Apple|         Computers|$277.00|Apple iMac MC509L...|\n",
      "|B00361EZUQ|    5.0| 09-6-2010| A36C22168X6UQ|After 25+ years o...|My first Mac, lov...|               Apple|         Computers|$277.00|Apple iMac MC509L...|\n",
      "|B00361EZUQ|    4.0| 09-6-2010|A1JUBRIZMO0RJV|I anxiously await...|Solid, Quick Mac,...|               Apple|         Computers|$277.00|Apple iMac MC509L...|\n",
      "+----------+-------+----------+--------------+--------------------+--------------------+--------------------+------------------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# udf with Regex\n",
    "\n",
    "\n",
    "# # Second pass to remove spaces - Doesn't appear to be working\n",
    "\n",
    "# First pass to remove commas and add dashes\n",
    "elect_df = elect_df \\\n",
    "    .withColumn('reviewTime', regexp_replace(col('reviewTime'),'(\\s|, )','-')) \\\n",
    "    .withColumn('reviewTime', trim(col('reviewTime'))) # Second pass to remove spaces - Doesn't appear to be working\n",
    "\n",
    "\n",
    "elect_df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting from string to date..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/06 18:31:19 ERROR Executor: Exception in task 0.0 in stage 28.0 (TID 548)]\n",
      "org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\n",
      "Fail to parse '09-2-2009' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseDateTimeInNewParserError(QueryExecutionErrors.scala:1368)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:149)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:176)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.time.format.DateTimeParseException: Text '09-2-2009' could not be parsed at index 3\n",
      "\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1949)\n",
      "\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:168)\n",
      "\t... 18 more\n",
      "23/06/06 18:31:19 WARN TaskSetManager: Lost task 0.0 in stage 28.0 (TID 548) (192.168.0.29 executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\n",
      "Fail to parse '09-2-2009' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseDateTimeInNewParserError(QueryExecutionErrors.scala:1368)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:149)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:176)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.time.format.DateTimeParseException: Text '09-2-2009' could not be parsed at index 3\n",
      "\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1949)\n",
      "\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:168)\n",
      "\t... 18 more\n",
      "\n",
      "23/06/06 18:31:19 ERROR TaskSetManager: Task 0 in stage 28.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o98.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 28.0 failed 1 times, most recent failure: Lost task 0.0 in stage 28.0 (TID 548) (192.168.0.29 executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to parse '09-2-2009' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseDateTimeInNewParserError(QueryExecutionErrors.scala:1368)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:149)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:176)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.time.format.DateTimeParseException: Text '09-2-2009' could not be parsed at index 3\n\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1949)\n\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:168)\n\t... 18 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:354)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:382)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3161)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3382)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:323)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to parse '09-2-2009' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseDateTimeInNewParserError(QueryExecutionErrors.scala:1368)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:149)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:176)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.time.format.DateTimeParseException: Text '09-2-2009' could not be parsed at index 3\n\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1949)\n\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:168)\n\t... 18 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m dt_format \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mMM-dd-yyyy\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m elect_df\u001b[39m.\u001b[39;49mwithColumn(\u001b[39m'\u001b[39;49m\u001b[39mreviewTime\u001b[39;49m\u001b[39m'\u001b[39;49m, to_date(elect_df\u001b[39m.\u001b[39;49mreviewTime, \u001b[39mformat\u001b[39;49m \u001b[39m=\u001b[39;49m dt_format))\u001b[39m.\u001b[39;49mshow()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mads/lib/python3.11/site-packages/pyspark/sql/dataframe.py:899\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[39mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    894\u001b[0m         error_class\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNOT_BOOL\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    895\u001b[0m         message_parameters\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39marg_name\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mvertical\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39marg_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mtype\u001b[39m(vertical)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[1;32m    896\u001b[0m     )\n\u001b[1;32m    898\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(truncate, \u001b[39mbool\u001b[39m) \u001b[39mand\u001b[39;00m truncate:\n\u001b[0;32m--> 899\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mshowString(n, \u001b[39m20\u001b[39;49m, vertical))\n\u001b[1;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    901\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mads/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mads/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    170\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mads/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o98.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 28.0 failed 1 times, most recent failure: Lost task 0.0 in stage 28.0 (TID 548) (192.168.0.29 executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to parse '09-2-2009' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseDateTimeInNewParserError(QueryExecutionErrors.scala:1368)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:149)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:176)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.time.format.DateTimeParseException: Text '09-2-2009' could not be parsed at index 3\n\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1949)\n\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:168)\n\t... 18 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:354)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:382)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3161)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3382)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:323)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to parse '09-2-2009' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseDateTimeInNewParserError(QueryExecutionErrors.scala:1368)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:149)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:176)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.time.format.DateTimeParseException: Text '09-2-2009' could not be parsed at index 3\n\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1949)\n\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:168)\n\t... 18 more\n"
     ]
    }
   ],
   "source": [
    "dt_format = \"MM-dd-yyyy\"\n",
    "elect_df.withColumn('reviewTime', to_date(elect_df.reviewTime, format = dt_format)).show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark does not infer the date format resulting in nulls. Using the format variable `dt_format` results in an error. I suspect the reviewTime column is not perfectly clean, tried stripping whitespaces...Is there a way to return nulls for improper conversions? Need dates for customer lifetime value."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A bit of exploration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many unique items are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "160052"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elect_df.select('asin').distinct().count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "160K unique items..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many unique customers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "728719"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elect_df.select('reviewerID').distinct().count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "725K unique customers..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns overview\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column Name</th>\n",
       "      <th>Data type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>asin</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>overall</td>\n",
       "      <td>float</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>reviewTime</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reviewerID</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>reviewText</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>summary</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>brand</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>main_cat</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>price</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>title</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Column Name Data type\n",
       "0        asin    string\n",
       "1     overall     float\n",
       "2  reviewTime    string\n",
       "3  reviewerID    string\n",
       "4  reviewText    string\n",
       "5     summary    string\n",
       "6       brand    string\n",
       "7    main_cat    string\n",
       "8       price    string\n",
       "9       title    string"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Columns overview')\n",
    "pd.DataFrame(elect_df.dtypes, columns = ['Column Name','Data type'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the frequency of item by category? (Customer Segment?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/06 18:32:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/06/06 18:32:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/06/06 18:32:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/06/06 18:32:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/06/06 18:32:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/06/06 18:32:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/06/06 18:32:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/06/06 18:32:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>main_cat</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>2389218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Computers</td>\n",
       "      <td>1502536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All Electronics</td>\n",
       "      <td>824547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Home Audio &amp; Theater</td>\n",
       "      <td>687968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Camera &amp; Photo</td>\n",
       "      <td>675708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Cell Phones &amp; Accessories</td>\n",
       "      <td>339949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Car Electronics</td>\n",
       "      <td>95838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Camera &amp;amp; Photo</td>\n",
       "      <td>72954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Home Audio &amp;amp; Theater</td>\n",
       "      <td>70595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Tools &amp; Home Improvement</td>\n",
       "      <td>52912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Office Products</td>\n",
       "      <td>47304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Musical Instruments</td>\n",
       "      <td>46724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Sports &amp; Outdoors</td>\n",
       "      <td>30506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Amazon Devices</td>\n",
       "      <td>26572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Automotive</td>\n",
       "      <td>16107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Amazon Home</td>\n",
       "      <td>14188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Industrial &amp; Scientific</td>\n",
       "      <td>12033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>GPS &amp; Navigation</td>\n",
       "      <td>8649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Cell Phones &amp;amp; Accessories</td>\n",
       "      <td>8597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Health &amp; Personal Care</td>\n",
       "      <td>8486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Portable Audio &amp; Accessories</td>\n",
       "      <td>8252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Tools &amp;amp; Home Improvement</td>\n",
       "      <td>6513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td></td>\n",
       "      <td>6433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>&lt;img src=\"https://images-na.ssl-images-amazon....</td>\n",
       "      <td>5732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Toys &amp; Games</td>\n",
       "      <td>3818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Sports &amp;amp; Outdoors</td>\n",
       "      <td>3288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Arts, Crafts &amp; Sewing</td>\n",
       "      <td>3148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Video Games</td>\n",
       "      <td>1759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Baby</td>\n",
       "      <td>863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Apple Products</td>\n",
       "      <td>648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Software</td>\n",
       "      <td>639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Industrial &amp;amp; Scientific</td>\n",
       "      <td>537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Books</td>\n",
       "      <td>518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>All Beauty</td>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Portable Audio &amp;amp; Accessories</td>\n",
       "      <td>372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Pet Supplies</td>\n",
       "      <td>261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Appliances</td>\n",
       "      <td>227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Amazon Fire TV</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Arts, Crafts &amp;amp; Sewing</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>&lt;img src=\"https://images-na.ssl-images-amazon....</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>GPS &amp;amp; Navigation</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Health &amp;amp; Personal Care</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>&lt;img src=\"https://m.media-amazon.com/images/G/...</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Gift Cards</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Fire Phone</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Toys &amp;amp; Games</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             main_cat    count\n",
       "0                                                None  2389218\n",
       "1                                           Computers  1502536\n",
       "2                                     All Electronics   824547\n",
       "3                                Home Audio & Theater   687968\n",
       "4                                      Camera & Photo   675708\n",
       "5                           Cell Phones & Accessories   339949\n",
       "6                                     Car Electronics    95838\n",
       "7                                  Camera &amp; Photo    72954\n",
       "8                            Home Audio &amp; Theater    70595\n",
       "9                            Tools & Home Improvement    52912\n",
       "10                                    Office Products    47304\n",
       "11                                Musical Instruments    46724\n",
       "12                                  Sports & Outdoors    30506\n",
       "13                                     Amazon Devices    26572\n",
       "14                                         Automotive    16107\n",
       "15                                        Amazon Home    14188\n",
       "16                            Industrial & Scientific    12033\n",
       "17                                   GPS & Navigation     8649\n",
       "18                      Cell Phones &amp; Accessories     8597\n",
       "19                             Health & Personal Care     8486\n",
       "20                       Portable Audio & Accessories     8252\n",
       "21                       Tools &amp; Home Improvement     6513\n",
       "22                                                        6433\n",
       "23  <img src=\"https://images-na.ssl-images-amazon....     5732\n",
       "24                                       Toys & Games     3818\n",
       "25                              Sports &amp; Outdoors     3288\n",
       "26                              Arts, Crafts & Sewing     3148\n",
       "27                                        Video Games     1759\n",
       "28                                               Baby      863\n",
       "29                                     Apple Products      648\n",
       "30                                           Software      639\n",
       "31                        Industrial &amp; Scientific      537\n",
       "32                                              Books      518\n",
       "33                                         All Beauty      510\n",
       "34                   Portable Audio &amp; Accessories      372\n",
       "35                                       Pet Supplies      261\n",
       "36                                         Appliances      227\n",
       "37                                     Amazon Fire TV      177\n",
       "38                          Arts, Crafts &amp; Sewing      117\n",
       "39  <img src=\"https://images-na.ssl-images-amazon....      113\n",
       "40                               GPS &amp; Navigation       93\n",
       "41                         Health &amp; Personal Care       71\n",
       "42  <img src=\"https://m.media-amazon.com/images/G/...       36\n",
       "43                                         Gift Cards       21\n",
       "44                                         Fire Phone        8\n",
       "45                                   Toys &amp; Games        3"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elect_df.groupBy('main_cat').count().orderBy('count', ascending = False).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
