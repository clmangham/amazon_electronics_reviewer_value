{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon Electronics Reviever Value: Data Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to assess “reviewer lifetime value” and larger trends in spend volume by analyzing reviews of Amazon purchases. \n",
    "\n",
    "The Amazon reviews dataset contains 233.1 million reviews across all categories of product. In our analysis we will only focus on a subset of the electronics category, which only contains about 6.7 million records (the full electronics dataset contains almost 21 million records).\n",
    "\n",
    "The datasets are available as compressed JSON files via download at: https://nijianmo.github.io/amazon/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import gzip\n",
    "import re\n",
    "\n",
    "# PySpark specific modules\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading these datasets into the typical Pandas workflow requires a sizable fraction of memory on our local machines and quickly becomes unwieldy. To prepare these large datasets for analysis, we leveraged distributed data processing with the PySpark API. Spark provides an analytics engine that represents a dataset as a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"Milestone_I\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transaction metadata\n",
    "\n",
    "**Amazon Electronics transaction metadata**: Includes descriptions, price, sales-rank, brand info, and co-purchasing links for 786,868 products. Key variables include the ID of the product, the product price in US dollars, brand name and the top category to which the product belongs.\n",
    "\n",
    "The downloaded file is in a compressed format. Compressed files can be loaded into pyspark, but won't beneifit from the data being partitioned to each core. We unzip the file with a shell script as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%sh\n",
    "# gzip -d meta_Electronics.json.gz\n",
    "# ls -lhtr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the first row of JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"category\": [\"Electronics\", \"Camera &amp; Photo\", \"Video Surveillance\", \"Surveillance Systems\", \"Surveillance DVR Kits\"], \"tech1\": \"\", \"description\": [\"The following camera brands and models have been tested for compatibility with GV-Software.\\nGeoVision \\tACTi \\tArecont Vision \\tAXIS \\tBosch \\tCanon\\nCNB \\tD-Link \\tEtroVision \\tHikVision \\tHUNT \\tIQEye\\nJVC \\tLG \\tMOBOTIX \\tPanasonic \\tPelco \\tSamsung\\nSanyo \\tSony \\tUDP \\tVerint \\tVIVOTEK \\t \\n \\nCompatible Standard and Protocol\\nGV-System also allows for integration with all other IP video devices compatible with ONVIF(V2.0), PSIA (V1.1) standards, or RTSP protocol.\\nONVIF \\tPSIA \\tRTSP \\t  \\t  \\t \\nNote: Specifications are subject to change without notice. Every effort has been made to ensure that the information on this Web site is accurate. No liability is assumed for incidental or consequential damages arising from the use of the information or products contained herein.\"], \"fit\": \"\", \"title\": \"Genuine Geovision 1 Channel 3rd Party NVR IP Software with USB Dongle Onvif PSIA\", \"also_buy\": [], \"tech2\": \"\", \"brand\": \"GeoVision\", \"feature\": [\"Genuine Geovision 1 Channel NVR IP Software\", \"Support 3rd Party IP Camera\", \"USB Dongle\"], \"rank\": [\">#3,092 in Tools &amp; Home Improvement &gt; Safety &amp; Security &gt; Home Security &amp; Surveillance &gt; Complete Surveillance Systems &gt; Surveillance DVR Kits\", \">#5,010 in Tools &amp; Home Improvement &gt; Safety &amp; Security &gt; Home Security &amp; Surveillance &gt; Surveillance Video Equipment\"], \"also_view\": [], \"main_cat\": \"Camera &amp; Photo\", \"similar_item\": \"\", \"date\": \"January 28, 2014\", \"price\": \"$65.00\", \"asin\": \"0011300000\", \"imageURL\": [\"https://images-na.ssl-images-amazon.com/images/I/411uoWa89KL._SS40_.jpg\"], \"imageURLHighRes\": [\"https://images-na.ssl-images-amazon.com/images/I/411uoWa89KL.jpg\"]}\n"
     ]
    }
   ],
   "source": [
    "N = 1\n",
    "with open(\"meta_Electronics.json\") as f:\n",
    "    for i in range(0, N):\n",
    "        print(f.readline(), end=\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By copying the output above we can [generate a schema](https://preetranjan.github.io/pyspark-schema-generator/) variable for the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"category\", ArrayType(StringType()), True),\n",
    "        StructField(\"tech1\", StringType(), True),\n",
    "        StructField(\"description\", ArrayType(StringType()), True),\n",
    "        StructField(\"fit\", StringType(), True),\n",
    "        StructField(\"title\", StringType(), True),\n",
    "        StructField(\"also_buy\", ArrayType(StringType()), True),\n",
    "        StructField(\"tech2\", StringType(), True),\n",
    "        StructField(\"brand\", StringType(), True),\n",
    "        StructField(\"feature\", ArrayType(StringType()), True),\n",
    "        StructField(\"rank\", ArrayType(StringType()), True),\n",
    "        StructField(\"also_view\", ArrayType(StringType()), True),\n",
    "        StructField(\"main_cat\", StringType(), True),\n",
    "        StructField(\"similar_item\", StringType(), True),\n",
    "        StructField(\"date\", StringType(), True),\n",
    "        StructField(\"price\", StringType(), True),\n",
    "        StructField(\"asin\", StringType(), True),\n",
    "        StructField(\"imageURL\", ArrayType(StringType()), True),\n",
    "        StructField(\"imageURLHighRes\", ArrayType(StringType()), True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The schema above is the result of the generator, and was edited to ensure specific fields are assigned the correct datatype. For example `StringType()` was added as parameter in some `ArrayType()` fields where `null` was a parameter. \n",
    "\n",
    "With the schema variable we quickly load the data into a spark dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_elect_df_raw = spark.read.format(\"json\").load(\"meta_Electronics.json\", schema=schema)\n",
    "meta_elect_df = meta_elect_df_raw\n",
    "# Uncomment to view schema\n",
    "# meta_elect_df.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the dataframe we can view the number of partitions the data has been split into:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_elect_df.rdd.getNumPartitions()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also confirm the dimensions of the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:========================================================>(81 + 1) / 82]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(786445, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print((meta_elect_df.count(), len(meta_elect_df.columns)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that a few hundred rows are missing (minor %). Website lists 786,868 products but here we have 786,445 records.\n",
    "\n",
    "Next we take a look at the first few rows of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+---+--------------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+------------------+------------+----------------+--------------------+----------+--------------------+--------------------+\n",
      "|            category|tech1|         description|fit|               title|            also_buy|tech2|               brand|             feature|                rank|           also_view|          main_cat|similar_item|            date|               price|      asin|            imageURL|     imageURLHighRes|\n",
      "+--------------------+-----+--------------------+---+--------------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+------------------+------------+----------------+--------------------+----------+--------------------+--------------------+\n",
      "|[Electronics, Cam...|     |[The following ca...|   |Genuine Geovision...|                  []|     |           GeoVision|[Genuine Geovisio...|[>#3,092 in Tools...|                  []|Camera &amp; Photo|            |January 28, 2014|              $65.00|0011300000|[https://images-n...|[https://images-n...|\n",
      "|[Electronics, Cam...|     |[This second edit...|   |Books \"Handbook o...|        [0999470906]|     |        33 Books Co.|[Detailed chapter...|[>#55,933 in Came...|[0943396670, 1138...|Camera &amp; Photo|            |   June 17, 2003|                    |0043396828|[https://images-n...|[https://images-n...|\n",
      "|[Electronics, eBo...|     |[A zesty tale. (P...|   |      One Hot Summer|[0425167798, 0399...|     |Visit Amazon's Ca...|                  []|                null|                  []|             Books|            |                |              $11.49|0060009810|                  []|                  []|\n",
      "|[Electronics, eBo...|     |                  []|   |Hurray for Hattie...|[0060219521, 0060...|     |Visit Amazon's Di...|                  []|                null|[0060219521, 0060...|             Books|            |                |.a-section.a-spac...|0060219602|                  []|                  []|\n",
      "|[Electronics, eBo...|     |[&#8220;sex.lies....|   |sex.lies.murder.f...|                  []|     |Visit Amazon's Lo...|                  []|                null|                  []|             Books|            |                |              $13.95|0060786817|                  []|                  []|\n",
      "+--------------------+-----+--------------------+---+--------------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+------------------+------------+----------------+--------------------+----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "meta_elect_df.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see our expected columns. This dataset has several interesting attributes, but for now we will select the columns we need for an assessment of reviewer value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+--------------------+--------------------+\n",
      "|      asin|               brand|          main_cat|               price|               title|\n",
      "+----------+--------------------+------------------+--------------------+--------------------+\n",
      "|0011300000|           GeoVision|Camera &amp; Photo|              $65.00|Genuine Geovision...|\n",
      "|0043396828|        33 Books Co.|Camera &amp; Photo|                    |Books \"Handbook o...|\n",
      "|0060009810|Visit Amazon's Ca...|             Books|              $11.49|      One Hot Summer|\n",
      "|0060219602|Visit Amazon's Di...|             Books|.a-section.a-spac...|Hurray for Hattie...|\n",
      "|0060786817|Visit Amazon's Lo...|             Books|              $13.95|sex.lies.murder.f...|\n",
      "+----------+--------------------+------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols_to_use = [\"asin\", \"brand\", \"main_cat\", \"price\", \"title\"]\n",
    "meta_elect_df.select(cols_to_use).show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at the price attribute above we can see that there are rows with missing values, and rows with non price values. We only want rows with prices so we will use regex to filter for values that start with a doller sign ($):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+------+--------------------+\n",
      "|      asin|               brand|            main_cat| price|               title|\n",
      "+----------+--------------------+--------------------+------+--------------------+\n",
      "|0011300000|           GeoVision|  Camera &amp; Photo|$65.00|Genuine Geovision...|\n",
      "|0060009810|Visit Amazon's Ca...|               Books|$11.49|      One Hot Summer|\n",
      "|0060786817|Visit Amazon's Lo...|               Books|$13.95|sex.lies.murder.f...|\n",
      "|0091912407|            ABBY LEE|               Books| $4.76|Girl with a One-t...|\n",
      "|0132492776|     Enter The Arena|Home Audio & Theater| $7.99|Wireless Bluetoot...|\n",
      "+----------+--------------------+--------------------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Regex for anything that starts with a dollar sign ($)\n",
    "expr = \"\\$.*\"  \n",
    "\n",
    "# Filter with expression\n",
    "meta_elect_df = meta_elect_df.filter(meta_elect_df.price.rlike(expr)).select(\n",
    "    cols_to_use\n",
    ")\n",
    "meta_elect_df.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the dimensions again after filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:=======================================================> (80 + 2) / 82]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(304323, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Check dimensions\n",
    "print((meta_elect_df.count(), len(meta_elect_df.columns)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About half of the dataset remains after filtering for only rows with prices."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon Electronics Reviews 5-core  (5-core)\n",
    "\n",
    "**Amazon Electronics Reviews 5-core dataset**: Contains 6,739,590 user reviews from 1999 to 2018. These data have been reduced to extract the k-core, such that each of the remaining users and items have k reviews each. Key variables include the ID of the reviewer, ID of the product and the time of the review.\n",
    "\n",
    "Same as before, we begin by decompressing the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%sh\n",
    "\n",
    "# gzip -d Electronics_5.json.gz\n",
    "# ls -lhtr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the first line of JSON: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"overall\": 5.0, \"vote\": \"67\", \"verified\": true, \"reviewTime\": \"09 18, 1999\", \"reviewerID\": \"AAP7PPBU72QFM\", \"asin\": \"0151004714\", \"style\": {\"Format:\": \" Hardcover\"}, \"reviewerName\": \"D. C. Carrad\", \"reviewText\": \"This is the best novel I have read in 2 or 3 years.  It is everything that fiction should be -- beautifully written, engaging, well-plotted and structured.  It has several layers of meanings -- historical, family,  philosophical and more -- and blends them all skillfully and interestingly.  It makes the American grad student/writers' workshop \\\"my parents were  mean to me and then my professors were mean to me\\\" trivia look  childish and silly by comparison, as they are.\\nAnyone who says this is an  adolescent girl's coming of age story is trivializing it.  Ignore them.  Read this book if you love literature.\\nI was particularly impressed with  this young author's grasp of the meaning and texture of the lost world of  French Algeria in the 1950's and '60's...particularly poignant when read in  1999 from another ruined and abandoned French colony, amid the decaying  buildings of Phnom Penh...\\nI hope the author will write many more books  and that her publishers will bring her first novel back into print -- I  want to read it.  Thank you, Ms. Messud, for writing such a wonderful work.\", \"summary\": \"A star is born\", \"unixReviewTime\": 937612800}\n"
     ]
    }
   ],
   "source": [
    "N = 1\n",
    "with open(\"Electronics_5.json\") as f:\n",
    "    for i in range(0, N):\n",
    "        print(f.readline(), end=\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy first row of JSON output above and [Generate Schema](https://preetranjan.github.io/pyspark-schema-generator/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\n",
    "            \"overall\", FloatType(), True\n",
    "        ),  # Changed to FloatType from StringType\n",
    "        StructField(\"vote\", StringType(), True),\n",
    "        StructField(\"verified\", BooleanType(), True),\n",
    "        StructField(\"reviewTime\", StringType(), True),\n",
    "        StructField(\"reviewerID\", StringType(), True),\n",
    "        StructField(\"asin\", StringType(), True),\n",
    "        StructField(\n",
    "            \"style\", StructType([StructField(\"Format:\", StringType(), True)]), True\n",
    "        ),\n",
    "        StructField(\"reviewerName\", StringType(), True),\n",
    "        StructField(\"reviewText\", StringType(), True),\n",
    "        StructField(\"summary\", StringType(), True),\n",
    "        StructField(\"unixReviewTime\", IntegerType(), True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use schema so spark does not have to infer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 core electronics data\n",
    "e5_core_df_raw = spark.read.json(\"Electronics_5.json\", schema=schema)\n",
    "e5_core_df = e5_core_df_raw\n",
    "# Uncomment to view schema \n",
    "# e5_core_df.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+--------+-----------+--------------+----------+-----------------+----------------+--------------------+--------------------+--------------+\n",
      "|overall|vote|verified| reviewTime|    reviewerID|      asin|            style|    reviewerName|          reviewText|             summary|unixReviewTime|\n",
      "+-------+----+--------+-----------+--------------+----------+-----------------+----------------+--------------------+--------------------+--------------+\n",
      "|    5.0|  67|    true|09 18, 1999| AAP7PPBU72QFM|0151004714|     { Hardcover}|    D. C. Carrad|This is the best ...|      A star is born|     937612800|\n",
      "|    3.0|   5|    true|10 23, 2013|A2E168DTVGE6SV|0151004714|{ Kindle Edition}|             Evy|Pages and pages o...|A stream of consc...|    1382486400|\n",
      "|    5.0|   4|   false| 09 2, 2008|A1ER5AYS3FQ9O3|0151004714|     { Paperback}|           Kcorn|This is the kind ...|I'm a huge fan of...|    1220313600|\n",
      "|    5.0|  13|   false| 09 4, 2000|A1T17LMQABMBN5|0151004714|     { Hardcover}| Caf Girl Writes|What gorgeous lan...|The most beautifu...|     968025600|\n",
      "|    3.0|   8|    true| 02 4, 2000|A3QHJ0FXK33OBE|0151004714|     { Hardcover}|W. Shane Schmidt|I was taken in by...|A dissenting view...|     949622400|\n",
      "+-------+----+--------+-----------+--------------+----------+-----------------+----------------+--------------------+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "e5_core_df.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e5_core_df.rdd.getNumPartitions()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:============================================>            (25 + 7) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6739590, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print((e5_core_df.count(), len(e5_core_df.columns)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe has expected number of rows. Now to select the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------+--------------+--------------------+\n",
      "|      asin|overall|unixReviewTime|    reviewerID|          reviewText|\n",
      "+----------+-------+--------------+--------------+--------------------+\n",
      "|0151004714|    5.0|     937612800| AAP7PPBU72QFM|This is the best ...|\n",
      "|0151004714|    3.0|    1382486400|A2E168DTVGE6SV|Pages and pages o...|\n",
      "|0151004714|    5.0|    1220313600|A1ER5AYS3FQ9O3|This is the kind ...|\n",
      "|0151004714|    5.0|     968025600|A1T17LMQABMBN5|What gorgeous lan...|\n",
      "|0151004714|    3.0|     949622400|A3QHJ0FXK33OBE|I was taken in by...|\n",
      "+----------+-------+--------------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols_to_use = [\"asin\", \"overall\", \"unixReviewTime\", \"reviewerID\", \"reviewText\"]\n",
    "e5_core_df = e5_core_df.select(cols_to_use)\n",
    "e5_core_df.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With both datasets loaded we can now merge the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge data \n",
    "\n",
    "We left join the data on the product ID number (`asin`), creating a new dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:======================================================> (80 + 2) / 82]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------+--------------+--------------------+-----+------------------+-------+--------------------+\n",
      "|      asin|overall|unixReviewTime|    reviewerID|          reviewText|brand|          main_cat|  price|               title|\n",
      "+----------+-------+--------------+--------------+--------------------+-----+------------------+-------+--------------------+\n",
      "|B000NOEDGK|    5.0|    1378339200|A195EZSQDW3E21|Few electronic pr...|Nikon|Camera &amp; Photo|$189.99|Nikon D40x 10.2MP...|\n",
      "|B000NOEDGK|    5.0|    1371945600|A31AX66K0TX9RE|I got this item f...|Nikon|Camera &amp; Photo|$189.99|Nikon D40x 10.2MP...|\n",
      "|B000NOEDGK|    5.0|    1369872000| A1SQ6W1WC94E4|After using my D4...|Nikon|Camera &amp; Photo|$189.99|Nikon D40x 10.2MP...|\n",
      "|B000NOEDGK|    5.0|    1363132800|A2FLZMVFJS231L|I've had this cam...|Nikon|Camera &amp; Photo|$189.99|Nikon D40x 10.2MP...|\n",
      "|B000NOEDGK|    5.0|    1314057600| A4EOR1UV8KU93|This is my second...|Nikon|Camera &amp; Photo|$189.99|Nikon D40x 10.2MP...|\n",
      "+----------+-------+--------------+--------------+--------------------+-----+------------------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "elect_df = e5_core_df.join(meta_elect_df, on=\"asin\", how=\"left\")\n",
    "elect_df.show(5)\n",
    "# print('5 core dataset: ', (e5_core_df.count(), len(e5_core_df.columns)))\n",
    "# print('Joined dataset: ', (elect_df.count(), len(elect_df.columns)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we check this new dataframe for missing values. (This can take a while, possibly minutes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:========================================================(32 + 0) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------------+----------+----------+-------+--------+-------+-------+\n",
      "|asin|overall|unixReviewTime|reviewerID|reviewText|  brand|main_cat|  price|  title|\n",
      "+----+-------+--------------+----------+----------+-------+--------+-------+-------+\n",
      "|   0|      0|             0|         0|      1380|2389218| 2389218|2389218|2389218|\n",
      "+----+-------+--------------+----------+----------+-------+--------+-------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "elect_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in elect_df.columns]\n",
    "   ).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see there are about 2.3 million rows that lack price data. Drop any rows with missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "elect_df = elect_df.na.drop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm missing values were dropped (can take a while, possibly minutes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:======================================================> (31 + 1) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------------+----------+----------+-----+--------+-----+-----+\n",
      "|asin|overall|unixReviewTime|reviewerID|reviewText|brand|main_cat|price|title|\n",
      "+----+-------+--------------+----------+----------+-----+--------+-----+-----+\n",
      "|   0|      0|             0|         0|         0|    0|       0|    0|    0|\n",
      "+----+-------+--------------+----------+----------+-----+--------+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "elect_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in elect_df.columns]\n",
    "   ).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check dimensions again after dropping missing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 48:====================================================>   (30 + 2) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# rows:  4585331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"# rows: \", elect_df.select('asin').count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.5 million records remain (down from 6.7 million)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean dates\n",
    "\n",
    "Next we clean the `unixreviewTime` column and extract the year and month to individual columns for ease of use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------+--------------+--------------------+--------------------+--------+------+--------------------+----+-----+\n",
      "|      asin|overall|unixReviewTime|    reviewerID|          reviewText|               brand|main_cat| price|               title|year|month|\n",
      "+----------+-------+--------------+--------------+--------------------+--------------------+--------+------+--------------------+----+-----+\n",
      "|0446697192|    5.0|    2009-07-13|A3LXXYBYUHZWS5|Fresh from Connec...|Visit Amazon's Zo...|   Books|$17.99|Hollywood Is like...|2009|    7|\n",
      "|0446697192|    5.0|    2009-07-09|A1X4L7AO1BXMHK|I don't know abou...|Visit Amazon's Zo...|   Books|$17.99|Hollywood Is like...|2009|    7|\n",
      "|0446697192|    3.0|    2009-09-01|A1Y9RUTH5GG3MU|Obviously the pre...|Visit Amazon's Zo...|   Books|$17.99|Hollywood Is like...|2009|    9|\n",
      "|0446697192|    4.0|    2009-08-29| AAR8E3JF9K93P|I am very happy t...|Visit Amazon's Zo...|   Books|$17.99|Hollywood Is like...|2009|    8|\n",
      "|0446697192|    4.0|    2009-07-28|A277GP2U2TXH51|\"Hollywood is Lik...|Visit Amazon's Zo...|   Books|$17.99|Hollywood Is like...|2009|    7|\n",
      "+----------+-------+--------------+--------------+--------------------+--------------------+--------+------+--------------------+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "elect_df = (\n",
    "    elect_df.withColumn(\n",
    "        \"unixReviewTime\", from_unixtime(col(\"unixReviewTime\"), \"MM-dd-yyyy\")\n",
    "    )\n",
    "    .withColumn(\"unixReviewTime\", to_date(col(\"unixReviewTime\"), \"MM-dd-yyyy\"))\n",
    "    .withColumn(\"year\", year(\"unixReviewTime\"))\n",
    "    .withColumn(\"month\", month(\"unixReviewTime\"))\n",
    ")\n",
    "\n",
    "elect_df.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Price\n",
    "Using regex, we remove the dollar sign from in front of prices and convert datatype from string to numerical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 43:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------+--------------+--------------------+--------------------+--------+-----+--------------------+----+-----+\n",
      "|      asin|overall|unixReviewTime|    reviewerID|          reviewText|               brand|main_cat|price|               title|year|month|\n",
      "+----------+-------+--------------+--------------+--------------------+--------------------+--------+-----+--------------------+----+-----+\n",
      "|0446697192|    5.0|    2009-07-13|A3LXXYBYUHZWS5|Fresh from Connec...|Visit Amazon's Zo...|   Books|17.99|Hollywood Is like...|2009|    7|\n",
      "|0446697192|    5.0|    2009-07-09|A1X4L7AO1BXMHK|I don't know abou...|Visit Amazon's Zo...|   Books|17.99|Hollywood Is like...|2009|    7|\n",
      "|0446697192|    3.0|    2009-09-01|A1Y9RUTH5GG3MU|Obviously the pre...|Visit Amazon's Zo...|   Books|17.99|Hollywood Is like...|2009|    9|\n",
      "|0446697192|    4.0|    2009-08-29| AAR8E3JF9K93P|I am very happy t...|Visit Amazon's Zo...|   Books|17.99|Hollywood Is like...|2009|    8|\n",
      "|0446697192|    4.0|    2009-07-28|A277GP2U2TXH51|\"Hollywood is Lik...|Visit Amazon's Zo...|   Books|17.99|Hollywood Is like...|2009|    7|\n",
      "+----------+-------+--------------+--------------+--------------------+--------------------+--------+-----+--------------------+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "elect_df = elect_df.withColumn(\n",
    "    \"price\", regexp_replace(\"price\", \"[$,]\", \"\").cast(\"double\")\n",
    ")\n",
    "# elect_df.printSchema()\n",
    "elect_df.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we [partition](https://sparkbyexamples.com/spark/spark-read-write-dataframe-parquet-example/) and save data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elect_df.write.partitionBy(\"year\").parquet(\"electronics_cleaned.parquet\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data is in a prepared state in which we can perform deeper analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Sample Datasets\n",
    "\n",
    "Next we create and export sample datasets (both raw and clean merged) for the reader to review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_elect_df_raw.limit(100).toPandas().to_json('meta_elect_df_sample.json')\n",
    "e5_core_df_raw.limit(100).toPandas().to_json('e5_core_df_sample.json')\n",
    "elect_df.limit(100).toPandas().to_json('clean_merged_df_sample.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
